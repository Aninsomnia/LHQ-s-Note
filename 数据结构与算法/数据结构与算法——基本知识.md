# 基本知识

* 浅拷贝和深拷贝：
  * 浅拷贝：**只是增加一个新指针，使其指向原有的内存地址。**如果内存内容发生改变，新指针指向的内容也会发生改变。
  * 深拷贝：**不仅增加一个新指针，同时也开辟了一块新的内存空间，用于存放被复制的内容。**
* 在涉及**考虑重复元素**的时候，往往要先对**候选数组进行排序**，这样可以**使得相同的数字相邻**，从而更为方便地处理重复元素带来的问题（例如**回溯算法中的重复剪枝**）。

# 二进制相关操作

### 取模和取余的步骤：

* 对A和B取模取余操作：A % B

1. C = A / B
2. R = A - C * B

* 取模和取余的区别仅仅是在计算C，也就是商的时候不同：

  * **取余**时：**商值向0靠近取整**，比如：

    ```
    C = A / B = 5 / 3 = 1.666666  靠近0取整，则C取 1 
    C = A / B = -7 / 2 = -3.5   ，靠近0取整，则C取-3
    ```

  * **取模**时：**商值向无穷小处取整**，比如：

    ```
    C = A / B = 5 / 3 = 1.666666，向无穷小处取整，则C取1
    C = A / B = -7 / 2 = -3.5   ，向无穷小处取整，则C取-4
    ```

* 可以看到，若A、B同符号，取模取余操作结果是相同。**所以在计算机中，对无符号数的取模和取余是一样的**。

* 简而言之：余数是基本数学运算（加、减、乘、除）中的相关概念；而取模是数论中的相关概念，二者不同。

* 一般在编程语言中的`%`都是**取余**符号。

### 一个无符号数对$2^n$取模（余）和该数对$2^n-1$做位与（&）操作结果相同

* 一个无符号数对$2^n$取模（余），就是**该数的二进制表示右移n位，结果为商，而右移的n位就是模数（余数）**。
* **$2^n-1$的二进制表示就是n个1**，一个无符号数和其做**位与（&）**操作，其结果和其对$2^n$做取模（余）操作是相同的。
* 可见，两者结果相同，但是**在计算机世界中，二进制的位操作的速度要远远大于加、减、乘、除、取模等操作的速度**，所以在浏览源码时，往往可以发现源码中，使用二进制的相关**位操作**代替其他**基本操作**。

# 线性表

#### 链表

* 单链表**无前驱节点时**的前插操作和删除操作
  * 前插：转换为后插，再将两个结点中的数据进行交换，**结果上是逻辑前插**
  * 删除：将后继节点数据复制到待删除结点，然后删除后继节点，**逻辑上是删除了前一个结点**
* **头插法可以用于链表的逆置！！！**
  * 因为头插法生成的链表顺序与数据的输入顺序**相反**，则常用于一个链表的**逆序**操作。
* **所有链表共同需要注意的问题：**
  * **如何判空**
  * **如何判断表头/表尾元素**
  * **如何插入删除（表头、表中、表尾）**
* 顺序表和链表的区别：
  * **插入和删除操作：**虽然两者的时间复杂度都是$O(n)$，但是顺序表消耗的时间大多是**移动元素**，链表消耗的时间大多是**查找元素**，而移动元素的开销要远远大于查找元素的开销，所以总体上讲链表的插入、删除操作要优于顺序表。
  * **按值查找**：顺序表有序时，可以通过折半查找等相关算法在$O(log_2n)$时间内找到，而链表无论有无序都得花费O(n)时间。

# 栈

* 顺序栈发生栈顶溢出时是非常危险的，因为顺序栈基于数组实现，而当数组越界时会产生缓冲区溢出的风险。
* 在使用所有链表时，**如果规定只从某一端插入/删除结点**，那么也可以将该链表看作为一个栈。**所以，使用头插法或者尾插法的链表就是一个链栈。**而且，**链栈不会出现栈满上溢的情况！！！！！！**

# 单调栈

* 单调栈实际上就是栈，只是利用了一些巧妙的逻辑，使得每次新元素入栈后，栈内的元素都保持有序**（单调递增或单调递减）**，若新元素入栈后发现破环了单调栈，那么就得把之前的栈顶元素弹出进行相关操作。
* 常用于解决**Next Greater Element**问题。
* 解决**Next Greater Element**问题通常有两种单调栈设计方式（以**“求数组右边第一个大于该元素的元素”**为例）：
  * 第一种：新元素是栈顶旧元素的**Next Greater Element**。**从左往右**遍历，维护一个从栈顶到栈底递增的单调栈，若**新元素大于栈顶旧元素**，说明**新元素就是栈顶旧元素的Next Greater Element**，然后将该旧元素出栈，继续比较。
  * 第二种，栈顶旧元素是新元素的**Next Greater Element**。**从右往左**遍历，维护一个从栈顶到栈底递增的单调栈，若**新元素小于栈顶旧元素**，说明**栈顶旧元素就是新元素的Next Greater Element**，此时栈依旧是单调栈。若新元素大于栈顶旧元素，说明栈顶旧元素不是新元素的Next Greater Element，就需要将旧元素出栈。
* 总的来说，新元素到来时，**永远先维护单调栈**。至于是维护过程中获得结果，还是维护完毕后获得结果，视情况而定。

# 堆栈区别

|      |                分配方式                |                           使用情况                           |                           释放方式                           |
| :--: | :------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|  堆  | 程序员手动分配，大小、结构由程序员决定 | 支持复杂的数据结构，在内存中向高地址拓展，存储空间并不连续。 | 通过函数库提供的函数手动释放；进程结束时由OS释放；Java虚拟机的垃圾回收机制自 动释放。 |
|  栈  |             由系统自动分配             | 只支持基本的数据结构但却是函数调用所依赖的结构，在内存中向低地址拓展，是连续的存储空间。 | 栈中变量生命周期结束时其栈空间自动释放，栈空间在线程结束时自动释放 |

# 队列

* 链式队列：实际上就是一个同时带有队头指针和队尾指针的单链表。
* 一般来说，**队头指针front一般指向队头元素，队尾指针rear一般指向队尾元素的下一个位置**。所以在初始化时，**两者都为同一位置。**

# 双端队列

* 两边都可以进行插入删除（入队出队）的队列。**所以，一般栈能实现的功能，双端队列都能实现。**

# 表达式

* 后缀表达式（运算符在操作数后面）
* 前缀表达式（运算符在操作数前面）

# 信息处理时的一类问题——逐层逐行处理

* 这类问题的解决方法往往是**在处理当前层（当前行）时就对下一层（下一行）做预处理，把下一层（下一行）的处理顺序安排好，等到当前层（行）处理完毕时，就可以依照已经安排好的处理顺序处理下一层（下一行）。**
* 例如：
  * **二叉树的层次遍历：**在遍历每一层时，**需要用队列将该层每一个结点的子结点依次进队**，那么，当处理完当前层时，**队列中下一层的结点就会依照着从左往右的顺序依次排好**（也就是说，**在处理上一层结点时，边处理，就已经边把下一层的处理顺序安排好了**）。
  * **图的广度优先遍历：**在遍历一个结点时，**需要用队列将该节点的未遍历的邻接结点入队**，那么，当处理完当前节点后，**队列中就是当前结点所有的未遍历邻接结点**（也就是说，**在处理完一个结点时，边处理，就已经边把它的邻接结点信息保存在队列中安排好了**）。
  * 二叉树的层次遍历与图的广度优先遍历区别：**树不存在回路，并且树的层次遍历每次只会遍历该结点的孩子节点，不可能到达已经访问过的结点**（也就是父亲结点和所有祖先节点）。但**图可能存在回路，并且一个结点的邻接节点中可能包含之前已经访问过的结点**。

# DFS、BFS

[BFS 的使用场景总结：层序遍历、最短路径问题 - 二叉树的层序遍历 - 力扣（LeetCode） (leetcode-cn.com)](https://leetcode-cn.com/problems/binary-tree-level-order-traversal/solution/bfs-de-shi-yong-chang-jing-zong-jie-ceng-xu-bian-l/)

### BFS

* **由近至远依次访问**和原点**有路径相通且路径长度为1、2、3......的顶点**。
* BFS是一种**分层的查找（搜索）**过程，**每向前走一步访问一批顶点**，不像DFS那样有回溯的情况，所以**不是**一种递归的算法。
* 算法使用一个**队列**，每遍历到一个结点，将该结点的所有邻接结点入队，然后依次遍历，（和树的层序遍历很类似）。
* BFS主要用于**层序遍历**或者**最短路径**问题：
  * 层序遍历：由于BFS是一种分层的查找，固然可以用于层序遍历。
  * 最短路径问题：我们可以**形象**地将BFS搜索过程看作为**“无线电波往外发散”**这样一个过程，“无线电”沿着**所有可能的路径**向外发散，**第一次碰到“某个物体”时**，便获得了**“发散源“到该”物体“的最短路径**。因为距离**”发散源点“**越近的**”物体“**，越先被**”探测到“**，故而在第一次遇见时，便是最短的情况。
* 可以看到，**「BFS 遍历」、「层序遍历」、「最短路径」**实际上是**递进**的关系。在 **BFS 遍历的基础上区分遍历的每一层**，就得到了层序遍历。在**层序遍历的基础上记录层数**，就得到了最短路径。

### DFS

* DFS可以看作一种**回溯**，例如**二叉树的前、中、后序遍历，遍历到叶子结点就依次向上返回**；求**AOV网的逆拓扑排序**中，将某一结点的所有出度结点访问完毕后，再来访问该结点，最终的访问顺序就是逆拓扑排序。

### DFS与BFS在边界条件上的处理

* DFS一般为了递归的美观以及代码的可读性，会**在被调用者栈帧中判断自己的边界条件**，这叫**“先污染后治理”**；而BFS一般会**在加入队列前，就判断该元素是否具备可以加入队列的条件**（例如该元素是否被访问过，被访问过就不得加入队列）。
* 其实无论是DFS还是BFS，**都可以选择在访问前或者是访问后**，二者中任意一个时刻来判断边界条件，两种方法都是符合代码逻辑的。**但是！！！！！！！！！！**对于BFS而言，**“加入队列”和“从队列中取出”本身就是较重的操作**，并且有时还会将**自定义类的实例**加入队列中，**如果此时还选择“先污染后治理”的方法，不仅过程上可能会导致同一元素重复加入队列的情况，并且中途耗费的无用时间也会太多，导致超时**。

# 树

* 树是一种**递归**的定义，在树的定义中又用到了自身。
* 树同时是一种**逻辑结构和分层结构**，适用于表示具有层次关系的数据。
* 在有n个结点的树中有n-1条边。
* **树其实也是一种从根结点开始的有向图。**

# 树的存储结构

### 双亲表示法

* 使用的性质：**任意树的任意结点，有且只有一个双亲结点。**
* 使用连续数组存储每个结点的值，数组下标为该结点的编号（值得注意的是：**下标（编号）并不一定要求和树的层次遍历次序相符合，各结点下标的顺序并不一定是各结点在树中的层次遍历顺序**），并且增设一个**“伪指针”**，用于存储其**双亲结点在数组中的位置下标（注意是下标）**。这意味着，**每一个结点要占用两个数组元素单元。**
* 通过任意结点$p$可以很方便地求出其双亲结点，但是欲求其孩子结点时，则需要**遍历整个数组**才能找到孩子结点，**这些结点的双亲结点下标为$p$的下标。**
* 与**二叉树顺序存储**的区别：
  * 树的顺序存储，下标为**编号（任意顺序）**。每个结点占用两个单位空间，一个存储结点值，一个存储**双亲结点下标**，用于表示各结点间的关系。
  * 二叉树的顺序存储，下标不仅为**编号（对应满二叉树的层次遍历编号）**，并且该编号就**足以表示各结点的关系**（比如**编号为`i`的结点，它的左孩子结点编号为$2i$，它的右孩子结点编号为$2i+1$，而它的父亲结点编号为$⌊i/2⌋$**），这同时也要求二叉树顺序存储必须**以$1$开始，而不是$0$**。每个结点占用一个单位空间，存储结点值。

### 孩子表示法

* **将每一个结点的孩子结点用一个单链表链接，并用数组将各个链表组织起来**（类似于处理hash冲突时使用的拉链法）。链表中每一个结点存放的是孩子结点在数组中的下标和下一个结点的指针。

### 孩子兄弟表示法（二叉树表示法）

* **这种存储结构和二叉树存储结构完全一致，只是结点中各个指针的含义不同。**其中，结点中**第一个指针指向的是第一个孩子结点，第二个指针指向的是下一个兄弟结点。**
* 若使用孩子兄弟表示法表示一棵树，实质上就完成了**树和二叉树相互转换**的操作。

# 二叉树

* **算数表达式的分析树的：①先序遍历就是前缀表达式②中序遍历就是中缀表达式③后序遍历就是后缀表达式**
* 二叉树的三种序列遍历递归算法虽然不同，但是**当抹去与递归无关的访问该节点关键字的语句后，三种递归完全一样**。也就是说，**从递归的执行过程来看，先序、中序和后序的结点遍历是完全相同的，只是访问该节点的关键字值的时机不同罢了**。
* 若**只给出二叉树的前/中/后/层序遍历的一种，是无法确定唯一的二叉树形态。**

### 后序遍历

* ```java
  if(root != null)
  {
  	postorder(root.left);
  	postorder(root.right);
  	//访问root结点
  }
  ```

* 可以看到，后序遍历**先访问两个孩子结点，再来访问该父亲结点。**

* 该思想很重要！！！很多时候，我们**必须知道两个孩子结点的情况，然后才能视情况返回该结点的返回值**。当我们需要由下向上回溯时，常常需要使用后序遍历的模板和思想。

# 二叉排序树（BST）

* 如果一个**二叉排序树同时也是一棵平衡二叉树**，那么在使用这棵二叉排序树时效率最高。
* 对二叉排序树进行**中序遍历**，得到的就是一个递增的有序序列。
* 最好情况下，二叉排序树**是一棵平衡二叉树，查找效率为$O(log_2n)$**，最坏情况下，二叉排序树**是一棵倾斜的单支树，查找效率为$O(n)$**。
* 从查找过程来看，二叉排序树查找和二分查找差不多。**但是二分查找的判定树唯一，而二叉排序树的查找不唯一，相同的关键字顺序不同可能生成不同的二叉排序树。**
* 当一个有序表是**静态查找表**时，**宜用顺序表作为其存储结构，并且采用二分查找实现其查找操作**。当一个有序表是**动态查找表**时，**宜选择二叉排序树作为其存储结构，并且采用二叉排序树的查找方法**。（这是因为如果查找表是动态的，对于二叉排序树而言**只需要修改和移动相应的指针就可完成插入删除操作，时间复杂度$O(log_2n)$**；而对于二分查找顺序表而言，**时间复杂度为$O(n)$**）

# 平衡二叉树（AVL）

* 每个结点的**左子树高度减去右子树高度**为该节点的**平衡因子**。故一棵平衡二叉树的每一结点的**平衡因子只能为-1、0、1。**
* 若插入一个新结点后树失去平衡，那么**离插入点最近的、平衡因子绝对值大于1的**结点为**最小不平衡子树的根。**
* LL：在结点A的**左孩子结点B**的**左子树**上插入了新结点，导致了A失去平衡。
  * 右单旋转：**将B右上旋转**代替A成为根结点，**将A右下旋转**成为B的右子树，**将B原来的右子树BR成为A现在的左子树**。
* RR：在结点A的**右孩子结点B**的**右子树**上插入了新结点，导致了A失去平衡。
  * 左单旋转：**将B左上旋转**代替A成为根结点，**将A左下旋转**成为B的左子树，**将B原来的左子树BL成为A现在的右子树**。
* LR：在结点A的**左孩子结点B**的**右子树**上插入了新结点，导致了A失去平衡。
  * 先左旋后右旋：**先将B的右孩子结点C左上旋转顶替B**，此时整体情况就变为了**LL**情况。然后**再将C右上旋转顶替A**。整个过程中，**原来C结点的左孩子变成了B结点的右孩子**，**原来C结点的右孩子变成了A结点的左孩子**，而**B、A结点分别成为了C结点的左右孩子。**
* RL：在结点A的**右孩子结点B**的**左子树**上插入了新结点，导致了A失去平衡。
  * 先右旋后左旋：**先将B的左孩子结点C右上旋转顶替B**，此时整体情况就变为了**RR**情况。然后**再将C左上旋转顶替A**。整个过程中，**原来C结点的右孩子变成了B结点的左孩子**，**原来C结点的左孩子变成了A结点的右孩子**，而**A、B结点分别成为了C结点的左右孩子。**
* 记$n(h)$为**深度为h的平衡二叉树的最少结点数**。那么：**n(h) = n(h-1) + n(h-2) +1（一个根节点，一个最小的高度为h-1的左子树，一个最小的高度为h-2的右子树）**。所以，如果需要求n(h)的值，需要采用**动态规划**方法来解决。

# 哈夫曼树（最优二叉树）

* 结点的权和带权路径长度：
  * 树中结点常常被赋予一个表示某种意义的值（或该结点的重要性等等），称为**该结点的权（注意是结点的权值，而不是边的权值）。**
  * 从根节点到某一结点的**路径长度与该结点权值的乘积**，称为该结点的**带权路径长度。**
  * 一棵树的**所有叶子结点的带权路径长度之和**，称为**该树的带权路径长度**。
* **$n$个带权叶子结点**组成的二叉树中，**带权路径长度最小的二叉树**，称为这$n$个叶子结点组成的**哈夫曼树**（最优二叉树）。
* 在构造哈夫曼树过程中，选择**两个根结点权值最小的树**，添加一个新的根节点，作为这两棵树的双亲，**将两者权值相加作为新的根节点的权值**，从而将两棵树合并为了一棵树。这样的构造过程保证了哈夫曼树**一定没有度为1的结点**。

# 哈夫曼编码

* 数据编码方式分为**固定长度编码**和**可变长度编码**，可变长度编码要比固定长度编码好用的多，因为可以**对频率高的字符赋以短编码，对频率低的字符赋以长编码**，从而使得整体的**平均编码长度减短**，起到**数据压缩**的作用。
* 将一个字符集中的每一个字符看作一个结点，字符出现的频率为该结点的权值，使用这些结点构造出一棵哈夫曼树，每一个字符结点处于叶子结点的位置。我们可以将某一个字符的编码，解释为**从根结点开始至该字符的路径上，边标记的序列（边标记为”0“意味着转向左孩子，为”1“意味着转向右孩子）**。这样，出现频率高的字符，一定会出现在整棵树的上层部分，频率低的字符，会出现在整棵树的下层部分，就可以获得一套关于该字符集的**平均编码长度最短的编码集**，并且该编码集是一种**前缀编码**集（即**没有一个字符的编码是其他字符编码的前缀**，这样的编码集解释出的结果是唯一的）。

# 动态规划和分治算法

* 分治主要是指**将问题划分为互不重叠、互不干扰的子问题**。而动态规划主要用于解决**“问题可以划分为子问题，但不同子问题之间又有子子问题的重叠”**这种情况。
* 动态规划分为两种：
  * **带备忘的自顶向下法：**此方法仍然按自然递归形式编写过程，但**过程中会保存每个子问题的解**（通常为数组或哈希表中）。当需要一个子问题的解时，会首先检查是否已经保存过此解。从而大大节省了计算时间。总之，**这个递归过程总会记住它计算过的所有解。**
  * **自底向上法：**该方法一般需要恰当定义子问题“规模”的概念，**使得任何子问题的求解都只依赖于规模更小的“子子问题”的求解**。所以我们就可以**将子问题按照规模排序**，**按照规模从小到大进行求解**，这样，**当求解到某个子问题时，它所依赖的所有更小的子问题都已被求解完毕**。这种方法的子问题只需要**求解一次**，那就是在我们第一次遇到它的时候。

# 回溯算法（back tracking）

* **回溯法**简而言之就是一种**递归的，暴力穷举法**，采用**试错**的思想，尝试**分步**地去解决问题。也就是说，当**为了获得某种解，必须要做出选择时，回溯算法会做出每一种选择**：

  * ①当最后发现该选择得不到解或者不是最优解时，将**会往回回溯，取消上一步或上几步的计算，撤销该选择，改为其他选择**。
  * ②当做出该选择最后得到了想要的解，那么**同样往回回溯，取消上一步或上几步的计算，撤销该选择，改为其他选择**。

* 所以：回溯的**本质是暴力枚举**（这意味着回溯算法的整体效率较低，但代码清晰易懂），而回溯的关键是如何**有规律、有逻辑的暴力枚举**，并且**在枚举失败时返回、在枚举成功时保存结果（边界条件）**。回溯则把暴力枚举的过程形象为一棵**树**（虽然没有使用树的的逻辑结构），**每一个结点有着自己的选择列表，选择列表中每一个选择被枚举后会延申出一棵子树。**

* **常用的剪枝函数：**用**约束函数**在扩展结点处剪去**不满足约束的子树**；用**限界函数**剪去**解不是最优解的子树**。

* 回溯的基本模板：

  ```java
  void backtracking(参数) {
      if (终止条件) {
          存放结果;
          return;
      }
  
      //更新本层状态
      for (选择：本层选择列表集合中元素（结点孩子的数量就是集合的大小）) {
          处理节点（或者做出选择）;
          //更新所选择的那一层状态;
          backtracking(路径，选择列表); // 递归
          重置状态;
          回溯，撤销处理结果（或者撤销选择）
      }
  }
  ```

* 算法在分步、尝试不同的解时，**做出选择后，需要改变当前的状态，不同结点的不同状态就代表着不同的解**。但是当发现某一选择的解不符合要求或者该选择下无解，而需要回溯时，**状态需要重新设置为选择前的状态，因此在回到上一层结点的过程中，需要撤销上一次的选择，并且进行状态重置**。这就是**“做出选择”和“撤销选择”、“更新状态”和“重置状态”**的含义。

### 回溯与DFS间的区别

* 回溯其实就是一种**深度优先遍历（DFS）**，在DFS每选择一条路径的时候，**做出选择、标记状态**，回溯返回的时候**撤销选择、重置状态**。从回溯树的根结点开始，沿着任意一条路径向下遍历，直到最底部的过程，被称为一次**完整的“选择”**。
* 不过回溯强调了在状态空间特别大的时候，**只用一份状态变量**去搜索所有可能的状态，**在搜索到符合条件的解的时候，通常会做一个拷贝**，这就是为什么经常在递归终止条件的时候，有 **`res.add(new ArrayList<>(path));`** 这样的代码。正是因为全程使用**一份状态变量**，因此它就有**「恢复现场」**和**「撤销选择」**的需要。
* 而**普通的DFS则不会使用一份全局状态变量**，传入下一级DFS的参数对象与当前DFS的参数对象并不是同一份变量，而是在当前DFS参数对象上稍作修改的新对象。

### 回溯问题的关键——选择与剪枝

* **在每个结点处，如何区分已选择的元素、未选择的元素（或者说如何在总集合中，提取出选择列表）。**
  * ①：若原集合的选择**本来就是从左往右（从一边到另一边）**，这时只需要简单的for循环，**左边的的元素就是已经选择过的的元素，右边的元素就是待选择的选择列表**。例如：求子集、求幂集。
  * ②：若原集合的选择**是具有全排列性质的，每个元素在每个位置都可能被选上**，这时可以设置一个**used判定表（通常为一个数组）和一个已经选择的元素个数usedNum**。**若某个元素被选择后，该元素在判定表中就为True，否则为false。选择之后，usedNum++，回溯时，usedNum--**。例如：求全排列。
* 对于剪枝把握：
  * 可以通过**对最后得到的解进行判断，看它是否可以被采纳**，不可被采纳就直接回溯，可被采纳就添加到结果集中。（形象的说就是**通过最后得到的果实来判断该长出该果实的树枝是否应该被剪枝**）。
  * 也可以在**该分支刚要生长的时候就对其判断**，不符合要求就立刻剪枝（形象的说就是**在该树枝刚要生长的时候就对其判断是否应该被剪枝**）

* **同一树层重复**和**同一树枝重复**：
  * 在**排列问题、子集问题**中常常会出现**“不允许重复”**的要求。例如在求幂集时，如果原集合中有重复数字，那么使用普通回溯算法求得的幂集中可能会存在重复的子集，这是不符合要求的。此时就**需要对回溯树进行剪枝**。
  * **同一树枝重复**：**在同一趟从根节点到叶子结点的选择过程中，某几个相同的元素，在不同树层被先后选择，导致的重复**。
  * **同一树层重复**：**在一个结点的选择列表中，某几个相同的元素，在该层被该结点先后选择，然后延申出各自独立的子树，导致的重复**。
  * 一般来说，**同一树层重复是不允许的，同一树枝重复是允许的**。将同一树层重复剪枝的方法很简单：设置数组**used[]**，在进行选择时，**将对应元素标记为`used[i] = true`，撤销选择时标记为`used[i] = false`**，每次进行选择时，使用`i`遍历当前选择列表，都要判断当前选择是否**`i > 0 && nums[i] == nums[i-1] && used[i-1] == false(或者这样：i > curNum && nums[i] == nums[i-1])`**，如果为`true`，说明这两个元素在同一树层的同一选择列表中，是兄弟关系；否则在同一树枝中，是父子关系（括号里面的去重方式的意思是：**`i > curNum`表明`nums[i]`和`nums[i-1]`位于同一结点的选择列表中**）。

# 图的最小生成树（Minimum-Spanning-Tree，MST）问题

* 对于一个图而言，保留所有结点和部分边，去掉其余部分边，使得**各结点之间互相连通，此时砍去一条边会使得图变为非连通图，加上一条边又会出现回路**，这种状态的图便称作为原图的**生成树**。
* 对于一个带权无向图，其所有的生成树中，**边权值最小**的那棵为该图的**最小生成树**。

# 图的最短路径问题

* 求最短路径算法通常依赖于一种性质：**两点之间的最短距离也包含了路径上其他顶点间的最短路径。最容易理解的就是：若v到u的最短路径是（v.....a......j u），那么v到j最短路径就是（v.....a.....j）。这其中包含着一种 ’递归‘ 思想，但是算法却不是递归算法。**
* 对于**无权图**而言，可以使用**BFS（广度优先遍历）**算法搜查查找**单源最短路径**。
  * 因为广度优先遍历具有**按照距离从近到远的性质**，**越先被遍历到的结点离初始结点越近**。
  * 使用广度优先遍历生成的生成树，是**深度最小**的。

### 迪杰斯特拉算法——求单源最短路径

* **核心思想**：将所有结点分为**①已求得最短路径的顶点集合S**和**②未求得最短路径的顶点集合V**。
  * 算法维护一个数组`dist[]`，`dist[j]`的值就是源点v到结点j的**当前最短路径（注意是当前最短路径，不是最终最短路径）**。
  * 每次遍历中，**将V中离源点v当前路径最近的（`dist[]`值最小的且处于集合V）、与S邻接的、结点u加入S中**，这个时候就**已经求出了v到u的最短路径（v到u的当前最短路径就变成了最终最短路径）**。
  * u加入S之后，u作为一个**中间点**，**会影响V中其余结点到源点v的距离**，所以此时会更新它们（就是更新数组`dist[]`，`dist[j] = min(dist[u] + w(u, j), dist[j])`），使得新V中的每个结点**到源点v的距离始终保持当前最短。**
* 如何标记某个结点**在集合S还是在集合V中？**
  * 使用一个**长度为n的布尔类型数组**`final[]`，如果某个结点**在已求得最短路径集合S中的话，对应的数组元素为true，否则为false。**
* 如何保存每个结点**距离源点的当前最短路径或者最终最短路径？**
  * 使用一个**长度为n的整数型数组`dist[]`**，保存着某个结点离源点的**当前最短距离**，若**该结点对应的final值为true**，那么保存的就是该结点离源点的**最终最短距离**。（final为true就表明**该结点已经算出最终最短距离并被加入到了集合S中**）。

# 并查集

* 在图论中，一张图可能存在多个连通分量，当我们需要获得图的连通分量个数时，**可以采用深度优先算法（DFS）或者广度优先算法（BFS）**遍历整个图表，**遍历的轮数就是连通分量的个数**。但是，当图的规模过于庞大时，由于在一次的DFS或者BFS中，**需要获得某个结点的所有邻接结点**，这个过程通常需要**较大时间开销**（邻接表存储：总共开销是$O(|E|)$，邻接矩阵存储：总共开销$O(|V|*|V|)$），那么对于整个图而言，需要遍历所有的结点，邻接表存储时的总时间复杂度为**$O(|E|+|V|)$**，邻接矩阵存储时总时间复杂度为**$O(|V|^2)$**。

* 此时可以使用**并查集**这样的一种数据结构来解决问题。我们通常**在开始时让每个元素形成一个单元素的集合**，之后再一步一步地**将那些应当属于同一分类的集合进行合并**，到最后，**只要是同一分类的元素都应当被置入同一集合中**。我们在每个集合中**选择一个元素充当该集合的代表元素**，想要查询某一个元素在哪个集合中实际就是查询**该元素所在集合的代表元素**，想要判断两个元素是否在同一个集合中，就是**判断这两个元素所在集合的代表元素是否相同**。

* 并查集的基本操作：

  * **初始化（Initialize）**：在开始时，将每个元素形成一个单元素的集合。这里我们选择一个数组`S[]`，**元素i的所在集合的代表元素为`S[i]`**。初始化时，`S[i] = i`。同时，**S[i] 也叫做元素i的父结点**。

  * **查找（Find）**：如果一个**元素`i`的父结点不是自己的话，那么说明该元素`i`不是所在集合的代表元素**（只有代表元素所在集合的代表元素是它自己，其余元素的代表元素绝不会指向自己，因为**并查集绝不会存在环**），不过该代表元素可以**通过查找元素i的父结点所在集合的代表元素**的方法找到（因为元素i和i的父结点同属于一个集合，其最终查找到的结果应该相同）。显而易见，这是一个**递归**的过程。

    ```java
    int find(int i)
    {
    	return S[i] == i ? i : find(S[i]);
    }
    ```

  * **合并（Union）**：两个集合想要合并，只需要**让其中一个集合的代表节点成为另一个集合代表节点的父结点**即可（两个帮派想要合并，只需要**一个帮派的老大成为另一个帮派老大的新老大就行了**）。

    ```java
    void union(int a, int b)
    {
    	S[find(a)] = find(b);//a的老大成为b的老大的小弟
    }
    ```

* **提高整体效率**

# 拓扑排序

### AOV网

* 若使用**DAG（有向无环图）来表示一个工程**，顶点表示活动，有向边表示活动间的先后顺序，那么这种图就被称为**AOV网**。
* **拓扑序列**：每个顶点只出现一次，并且**若序列中A在B的前面，那么AOV网中只会存在从A到B的路径，不会存在从B到A的路径。**（也就是说拓扑序列中**各顶点的先后顺序**表示着这些活动**执行的先后顺序**）。
* **拓扑排序**：
  * AOV中那些**没有入度的顶点一定是先要执行的活动**；那些有入度的顶点，一定代表着**要等到它们的前驱活动执行完毕后才能执行的活动**。所以拓扑排序的主要思想就是找出那些需要先执行的活动（**那些入度为0的顶点**），然后执行它们（**添加到拓扑序列中**）。
  * 拓扑排序同样可以使用**DFS算法**实现，同样DFS可以实现逆拓扑排序，这里只写后者。DFS算法中，在遍历完一个结点后，会找到与该结点，邻接的、未访问过的所有结点。假设一个结点入度为1，出度为3，那么当算法访问到该结点后，会依次：对第一个出度方向访问到底、回溯上来；对第二个出度方向访问到底，回溯上来；对第三个出度方向访问到底，回溯上来。此时，**该结点所有的邻接结点都已访问完毕**（换言之**该活动的所有后继活动都已执行完毕**），它**自己也将回溯**。那么，就可以将该结点（活动）添加到逆拓扑序列中（因为**它的后继活动都已执行完毕，它可以在逆拓扑序列中排在它们后面**了）。全部DFS执行完毕后，得到的就是该AOV网的逆拓扑排序序列。
  * 其实将DFS得出的逆拓扑排序序列**转置**一下就是拓扑排序了（或者将逆拓扑排序进栈，再出栈）。（哈哈哈哈哈）

### AOE网

* 若使用带权DAG来表示一个工程，**顶点表示各个事件，有向边表示各个活动，边的权重表示该活动消耗（持续）的时间**，那么这种图就叫做**AOE网**。

* **关键路径**：从源点到汇点的**最长**的一条路径叫关键路径。因为该路径最长，那么当该路径上所有的活动完成时，其余的所有活动一定已经完成。这条关键路径的长度就是**“要完成整个工程至少需要的时间”**。

* 对于每个事件$v$：

  * **最早开始时间ve(v)：从源点到该事件的最长路径长度，为该事件的最早开始时间**。意思是说必须等它之前耗时最长的活动都进行完毕后，才能触发该事件。
  * **最晚开始时间vl(v)：汇点事件的最早开始时间与该事件到汇点的最长路径长度之差**。假设汇点事件的最早开始时间为`ve(y)`，该事件到汇点的最长路径为`c(p)`，说明者从该事件开始，到汇点至少消耗`c(p)`的时间。为了不使汇点事件最早开始时间延后，该事件必须在`ve(y)-c(p)`之前触发，否则汇点的最早开始时间就会被拖后。

* 对于每个活动$a_i = <v, w>$：

  * **最早开始时间e(i)：前驱事件v的最早开始时间**。想要活动$a_i$尽早开始，那么其前驱触发事件v必须也尽早触发。
  * **最晚开始时间l(i)：后驱事件w的最晚开始时间与该活动耗时之差**。因为该活动不能影响w事件的最晚开始时间。

* 对于每一个活动$<v, w>$：**$ve(w) = max( ve(v) + c(<v,w>) )$；$vl(v) = min( vl(w) - c(<v, w>))$**使用动态规划思想表示为：

  ```java
  dp[j] = max{dp[i] + weight(i, j)}
  dp[i] = min{dp[j] - weight(i, j)}
  ```


# 查找（search）

* 一个查找表：若只需要进行查找操作，则为**静态查找表**；若不止需要进行查找操作，还需进行添加（删除）操作，那么该表为一**动态查找表**。（举个例子：**二叉排序树**就是一个典型的动态查找表，但如果不需要动态地插入删除操作，那么使用**二分查找表**也可完成查找操作）
* 平均查找长度包括**查找成功时的平均查找长度**和**查找失败时的平均查找长度**。
* 顺序查找：
  * 一般线性表的查找并未有啥特别之处，唯一值得注意的是，在遍历整个数组时，为了**避免不必要的判断语句（比如判断当前下标是否越界）**，可以引入**”哨兵“**的概念。让**线性表的头部留出一个空位**，每次要查找时，将**目标关键字放在这个空位中作为”哨兵“**。程序从另一边开始遍历，中间过程**不必判断下标是否越界**，只需**判断当前下标对应值是否等于目标关键字的值**。①若查找成功，那么**在中途就可以获得查找成功时的数组下标**。②若查找失败，那么当程序遍历到哨兵时，一定会安全退出，**此时下标为0，也意味着查找失败**。

# 折半查找（二分查找）

* 折半查找的判定树根节点的左子树要么和右子树**相同高度**，要么比右子树**高度小一**。也就是说，折半查找的判定树一定是棵**二叉平衡排序树**。且树和子树都**不会出现右子树高度大于左子树的情况。**
* 折半查找判定树的形态**只与查找表的元素个数`n`相关，与各个元素的值无关**。
* 折半查找非常重要的一点，当查找失败，循环退出时，**永远**都是左边的high指针，紧接着右边low指针：`[high][low]`，所以：
  * 要求获得**第一个大于key**的数：那么就要**返回low对应的值**。为了与查找成功时情况相统一，需要在**当`arr[mid] == key`时，`high = mid-1`**。此后的循环**high指针永远不会动**（因为后面的mid只会小于key），low指针会不停地向右移动直至跳出循环。
  * 要求获得**最后一个小于key**的数：那么就要**返回high对应的值**。为了与查找成功时情况相统一，需要在**当`arr[mid] == key`时，`low = mid+1`**。此后的循环**low指针永远不会动**（因为后面的mid只会大于key），high指针会不停地向左移动直至跳出循环。

# 分块查找（索引顺序查找）

* 除了数据表以外，再设置一个**索引表**。将数据表均分成很多块，每一块中的元素不一定有序，但是**前一块的最大值一定小于后一块的最小值**，即要求整个数据表是**”分块有序“**的。索引表中每一项存放对应数据块的**最大元素**和**该数据块的起始地址**，然后将索引项从小到大存放在索引表中。

* 首先查找索引表，由于索引表有序，可以采用**折半查找**的方式找到待查元素在哪一块中；然后在已确定的块中使用**顺序查找**找出待查元素**（因为每一块中的元素是无序的，只能采用顺序查找）**。

* 在该折半查找里面有一个特别特别特别值得注意的地方！！！！！！！！！！！！！！！！

  ```java
  int low = 0;
  int high = b-1;
  int mid;
  while(low <= high)
  {
  	mid = (low+high)/2;
  	if(I[mid].key >= key)
  		high = mid-1;
  	else
  		low = mid+1;
  }
  ```

  在上面这段代码中：

  * **若目标key刚好就是某个块的最大元素（key刚好被包含在索引表项中）：**当mid下标对应的索引项值刚好为key时，并没有直接跳出循环，获得目标块的起始地址`I[mid].link`，而是**继续将high的值设置为mid-1**，这样，接下来的分块查找，由于high对应的值都小于key（在key的左边一格位置），那么新的mid对应的值也会小于key，那么后面只会low不停地等于mid+1，到最后，high<low，表面上查找失败，但其实high+1（或者low）永远都是目标块的索引项。

  * **若目标key不被包含在索引表项中：**最终折半查找会查找失败（high<low），此时①要么目标key在high+1（或者low）那一索引项对应的数据块中，②要么high+1（或者low）超出数组范围，越界。

  * 这样做的目的很简单：**统一”获得目标块起始地址“这一功能的代码**，由于当查找失败时，我们是**向上取值（若A<key<B，我们取B**），所以这样做使得**目标块的起始地址统一为high+1（或者low）**：

    ```Java
    if(low == b)
    	return false;//目标关键字直接超出了索引表的范围，那么在数据表肯定不会找到了
    int beginning = I[low].link;
    ```

* 若索引表使用折半查找，则平均查找长度为$log_2(b+1)+s/2$，此时**每块长度越小越好。**

* 若索引表使用顺序查找，则平均查找长度为$(b+s)/2+1$，此时**当$s=\sqrt{n}$时效率最好。*

# B树（多路平衡查找树）

* **定义与性质**：
  
  * B树首先是一颗**排序树**。
  * B树中，所有结点的**孩子数量的最大值**称为该B树的阶，也就是每个结点最多可以有多少个孩子结点。
  * 一个m阶B树，每个结点**最多有m棵子树**，**最多有m-1个关键字**。
  * 一个m阶B树，除根结点以外，每个结点最少有**⌈m/2⌉**这么多个子树，最少有**⌈m/2⌉-1**这么多个关键字（有`i`个关键字，就有`i+1`棵子树）。不然B树极端情况下**就会退化成普通的二叉排序树**。
  * 每个结点的每棵子树高度完全一致，每个叶子结点都出现在同一层（说明**整棵B树是一棵完全平衡的多路查找树）**。这和平衡二叉树道理一样，为了**提高查找的效率**。
  * 和以往的树不同，B树的**叶子结点**指的是那些**查找失败的空结点**，其父结点叫做**终端结点**。
  * 每一个结点中：子树0<关键字1<子树1<关键字2<子树2<关键字3<子树3······
  * 一个含有n个关键字的m阶B树，最小高度为$log_m(n+1)$。
  * 有**n个关键字**的B树**必然会有n+1个叶子结点**（失败结点、外部节点）。（也就是说这n个关键字将数轴划分为了**n+1个区间**，若想要查找的值在这n个点上，查找成功，若想要查找的值在这n+1个区间上，则查找失败。
  
* **B树的查找**：
  
  * B树的查找可以**类比二叉排序树的查找**，主要分为两个步骤：①：在B树中寻找结点②：在结点内部找关键字
    * 时间花费主要是搜索结点上，即主要取决于**B树的高度**。
    * 在结点内部搜索关键字时，由于内部关键字序列是有序排列的，故**既可以使用顺序查找也可以用二分查找**。
  
* B树的插入：

  * （1）：**定位。**利用B树查找算法，找出**要插入该关键字的最底层的终端结点**，同时也会**确定查找失败的叶子结点**，也就是确定了在该终端结点内的插入位置。
  * （2）：**插入。**在查找到的终端结点中插入该关键字。然后判断该终端结点内部关键字个数**是否大于`m-1`**。若大于，则表明关键字个数多了，需要进行**分裂**。
  * （3）：**分裂。**当一个结点内部关键字个数过多时，需要对其进行分裂。在该结点从中间位置，**即⌈m/2⌉处，把结点分成两部分**。**左半部分的关键字保留在该结点中**；而对于右半部分的关键字，需要**新创一个结点，将它们放进新结点中**；对于⌈m/2⌉处的这个结点，需要**将它连同刚刚新结点的存储位置一起插入到父结点中**。若父结点关键字也超过了上限，也需要对父结点进行分裂操作，**当该过程传递到根节点时，B树的高度就会+1**。

* **B树的删除**：

  * 同样需要使用查找算法，**定位**该关键字所在结点。

  * 当被删关键字**k不在终端结点时**，可以使用**k的前驱（或后继）k‘来顶替k的位置**，然后**在相应的结点删除k’**，而k‘**一定是在某个终端结点中**，所以情况就转换成了删除终端结点关键字的操作。（这里和二叉排序树的删除既有相同点又有差别）

    |                       二叉排序树的删除                       |                          B树的删除                           |
    | :----------------------------------------------------------: | :----------------------------------------------------------: |
    | 需要判断待删除结点是否①无孩子结点、②只有左孩子，没有右孩子、③只有右孩子，没有左孩子、④左右孩子都有。四种情况分别对应四种不同处理方式。 | 也需要判断待删除关键字所在结点是否有孩子节点，不过只有**两种**情况：**①该结点为终端结点、②该结点不为终端结点。** |
    | 当待删除结点k左右孩子都有时，**需要使用k的前驱（或后继）k’来顶替k的位置，同时在左子树（或右子树）中删除k‘**，这是一个**递归**的过程。 | 当待删除关键字所在结点不为终端结点时，**也可以使用k的前驱（或后继）k‘来顶替k的位置，然后在k’所在的终端结点中删除k‘**，这也可以看作一个递归的过程。 |

  * 当待删除关键字在终端结点时：

    * 若该终端结点**关键字个数大于⌈m/2⌉-1**，那么意味着删除掉目标关键字后，仍满足B的定义和性质，那么**直接删除**掉待删关键字即可。
    * 若该终端结点**关键字个数等于⌈m/2⌉-1**，那么一位置如果直接删除掉目标关键字，该终端结点关键字个数将不满足B树定义，此时又分为两种情况：
      * 兄弟够借：若**其左（或右）兄弟个关键字个数大于⌈m/2⌉-1**，那么将左（或右）兄弟结点中最大（或最小）的关键字上移到父结点中，再将父结点中的那个大于（或小于）上移关键字的那个关键字下移到该结点中。也就是说，将**该结点的前驱（或后继）关键字从父结点中拿下来，再将前驱的前驱（或后继的后继）从兄弟节点中填补父结点中的空位。**
      * 兄弟不够借：若**其左（或右）兄弟个关键字个数全都等于⌈m/2⌉-1**，那么就需要将**删除关键字后的该结点**和左（或右）兄弟结点进行合并操作，这与B树的插入中的分裂操作相对应。合并时，**两个子结点、以及父结点中分割二者的那个关键字全都合为一个结点。**也就是说，合并后，**父结点关键字数量会减一**。若此时父结点不满足B树的定义，那么继续将父结点和父结点的兄弟结点进行合并，直至结束。**若最后根结点个数为0，那么整棵B树高度减一。**

# B+树

* 定义与性质：

  | B树                                                          | B+树                                                         |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 每个分支结点最多m棵子树，**最多m-1个关键字**                 | 每个分支结点最多m棵子树，**最多m个关键字**                   |
  | 非终端根结点至少两棵子树                                     | 非叶根结点至少两棵子树                                       |
  | 其余结点至少⌈m/2⌉棵子树                                      | 其余结点至少⌈m/2⌉棵子树                                      |
  | **关键字个数 = 子树个数-1**                                  | **关键字个数 = 子树个数**                                    |
  | 绝对平衡，所有叶子结点位于同一层                             | 绝对平衡，所有叶子结点位于同一层                             |
  | **所有叶子结点不带信息，为空**                               | **所有叶子结点包含关键字以及指向相应记录的指针，并且相邻叶子结点按大小顺序互相链接起来。** |
  | 所有非叶结点包含的是**指向子树的指针，以及分割这些子树区间的关键字**，并且**每一个关键字对应着一个记录的存储地址** | 所有非叶结点包含的是**指向子树的指针，以及对应子树的最大关键字**，**不含有该关键字对应记录的存储地址**（也就是说非叶节点仅仅是一个**索引表项，仅起索引作用**罢了） |
  | **任意两个结点内的关键字是不会重复的**                       | **所有叶结点的关键字就是全部的关键字**，也就是说**非叶节点中出现的索引项关键字也会出现在叶结点中** |
  | 通常只有一个头指针，指向根节点                               | 通常有**两个指针**，**一个指向根节点，一个指向关键字最小的叶子结点**。 |
  
  所以：B+树其实是一棵**索引树**，**类似于分块查找，每一个非叶节点都是其所有子树的索引表**，里面存放着每个对应子树的**最大值以及指向下一级索引块的指针**。而叶子结点则是一个**基本索引块**，它的指针不再指向下一级索引块，而是**直接指向数据文件中的记录**。
  
* B+树的查找：

  * B+树有两种查找方式：**①直接从最小关键字开始进行有序列表的顺序查找。②从根节点开始的多路查找。**不过在在多路查找过程中，**在分支结点上找到与给定值相同的关键字时并不像B树查找那样，会停止查找返回结果，而是会继续向下查找，直到叶子结点为止**。所以无论查找成功与否，**每次查找都会经过一条从根节点到叶子结点的路径。**

* B+树的插入、删除操作和B树插入、删除操作类似。

* B+树相对于B树的优势：

  * 通常来讲，一个结点是存放在**一个磁盘块**当中，每次从磁盘中**将一个磁盘块读取到内存**，然后对其中的关键字进行分析。
  * 在B+树中，非叶结点**不含有该关键字对应记录的存储地址（B树有）**，并且一个结点**最多可以存放m个关键字（B树最多只有m-1个）**，所以，**每一个磁盘块B+树可以包含更多的关键字**，这样，B+树的**阶更大**，树的**高度更小**，**磁盘的读取次数也会更少，总体的查找效率也会更好**。

# 散列查找（哈希查找）

* 无论是使用线性表的顺序查找，折半查找还是使用树表的二叉排序树查找，B树查找等算法，**记录在表中的位置与记录的关键字之间并不存在某种确定的关系**，因此在查找时需要进行一系列的**关键字比较**，即**这类查找方法是建立在”比较“的基础上**的，**查找的效率则取决于比较的次数**。

* 散列函数（hash函数）则是一种可以**把元素的关键字**映射为**该关键字在查找表中对应地址**的映射函数。

* 散列表（hash表）则是一种可以**根据关键字进行直接访问（随机存取）**的数据结构。

* 装填因子或者负载因子（Load Factor）则是**表中元素的个数与hash表长度的比值**。（例如java中hashmap的负载因子上限为0.75）

  * 一方面，负载因子越大，hash表中**空闲区域比例就越小，发生hash冲突的几率就越大**。
  * 但是另一方面，负载因子越大，hash表的**存储空间利用率就越大**。

  所以为既兼顾减少冲突的发生，又兼顾提高存储空间利用率，通常会**使负载因子控制在0.6~0.9范围内**，所以hash查找这样的一类算法，是典型的**”空间换时间“**的例子。

* 影响hash表查找的性能的因素主要有三种：

  * **负载因子：负载因子越大，hash冲突几率越大，效率越低**。
  * **hash函数本身：**hash函数选择得当，可以**使hash地址均匀地分布在hash地址空间中**，减少冲突的发生。
  * **解决hash冲突的方法**

* 常见的hash函数：

  * 直接定址法：
    * **直接使用关键字或者关键字的线性运算结果**来表示存放地址。适用于关键字分布较为连续的情况，并且使用这种hash函数是**不会产生hash冲突**的情况。
  * 除留余数法：
    * 假定hash表长度为m，选定**一个不大于m但最接近m或者等于m的质数p**，那么一个元素的存储位置就是**该元素关键字对p求模运算得出的结果**。使用质数的目的就是为了让关键字在表中的分布**更为均匀**。
  * 数字分析法：
    * 提取关键字中**取值较为均匀的数字作为hash地址**。例如电话号码的**后四位要比前三位，数字的分布更为均匀**。

* 处理hash冲突的办法：

  * **拉链法**：将那些hash值相同的同义词存放在一个**线性链表**中，该链表**由其散列地址唯一标识**，同时在**hash表对应的地址中存放链表的头指针**。（例如java中的hashmap，其数组中存放的就是一个单链表）

  * **开放定址法**：即存放新表项的空间，**既向它的同义词表项开放，也向它的非同义词表项开放**。或者说就是在同义词出现hash冲突的时候，**在hash表中找一个新的空闲位置存放同义词**。其数学递推公式为：**$H_i = (H(key) + d_i) \bmod m$**。（其中**m为hash表表长，不是除留余数法中的质数**；**$d_i$为增量序列**，不同的增量序列对应着不同的开放定址法）

    * **线性探测法：$d_i = 0,1,2,3······$**

      这种方法的特点是**当冲突发生时，顺序地查看表中的下一个单元，直到找出一个空闲单元**，若表未满，则一定可以找到一个空闲单元。这样同时也意味着第i个散列地址的同义词存在第i+1个散列地址中，这样本应该存入第i+1个散列地址的元素就争夺第i+2个散列地址，如此传递下去······从而**造成大量元素在相邻的散列地址上”聚集（或堆积）“起来，极大地降低了查找效率**。（产生**非同义词冲突**，就是**hash值不同的两个元素**也会为了**争夺同一个后继hash地址**导致出现堆积）

    * **平方探测法：$d_i = 0,1,-1,4,-4,9,-9······k_2,-k_2$，$k<=m/2$**

      使用平方探测法可以很好地**避免出现”堆积“的情况，从而避免大量冲突**，不过弊端就是不能探测到表中的所有单元。使用平方探测法时，表长必须为**$4k+3$**，才能保证所有存储单元都能被探测到。

    * **伪随机探测法：$d_i$是一个伪随机序列。**

# 排序

* 稳定性：若待排序序列中有两个元素关键字相同，那么**如果排序后两者相对位置不变，则排序是稳定的。**
* 内部排序：排序期间元素**全部存放在内存**。
* 外部排序：排序期间元素**无法全部同时存放在内存中**。

# 插入排序

* **将一个待排序的元素按关键字大小，插入到前面已经排好序的序列的适当位置，直到所有元素插入完成。**

### 直接插入排序

* 排序任意时刻，整个区间被分成两个子区间，前一个子区间是**已经排好序的有序区间**，后一个子区间是**当前未排序的无序区间**。每次操作都**将无序区的开头元素插入到有序区的适当位置**。
* 每趟产生的有序区不一定是**全局有序区**。

### 折半插入排序

* 插入排序中，都会进行两项工作：①**从前面的有序区找出待插入元素应该被插入的位置**；②**给插入位置腾出空间**
* 若第一项工作使用的是折半查找算法，那么**比较元素的次数就会变少，但是移动元素的次数并未减少**，故而折半插入排序的时间复杂度**依然为$O(n^2)$**。

### 希尔排序

* 由直接插入排序可知：当一个序列**已经有序、或者接近有序**时，使用直接插入排序的效果将较为优秀。而希尔排序的主要思想就是**将待排序列表分为一个个子表**，先对每一个子表**内部进行直接插入排序**，然后**两个子表合并之后的子表就是一个”较为有序“的序列**了，重复此操作，直至**最后整个列表都是”较为有序“**，那么对这时**对整个列表进行直接插入排序**将会快捷很多。

* 过程中，取每次循环**子表的数量为增量d**，**每次增量为前一个增量的1/2，最后增量变为1**。所以，希尔排序又被称为”**减少增量排序**“

* 同时也要注意一点，最外层循环是**①增量减少的循环**，然后是**②在同一增量下处理每一个子表的循环**，最后是**③在同一子表内进行插入排序的循环**。在第二个循环中，并不是一个子表处理完毕后再去处理其他子表，而是**从下标为d+1的元素开始、向右依次地、直到数组末尾、递增地，交替处理各个子表**（因为下标从1到d的元素都是每个子表的首元素，它们的位置在插入排序算法中是不变的）。而在第三个循环中，**`A[j]`就会和该子表内部`A[j-d]`、`A[j-2d]`·······等元素进行比较，从而实现插入排序**。

* 时间复杂度上，平均为$O(n^{1.3})$。为什么希尔排序的性能要优于直接插入排序呢？首先是两个显而易见的事实：

  * ①直接插入排序在**序列接近正序时时间消耗较少**。
  * ②当n值较小时，$n$与$n^2$差别不大，即**在n值较小时直接插入排序的最好时间复杂度$O(n)$与最坏时间复杂度$O(n^2)$差别也不大**。

  所以，在希尔排序开始时，**增量d较大，子表数量较多，而每个子表的元素数量较少**，所以**此时各子表的直接插入排序较快**；而之后**增量d变小，子表数量变少，各子表内元素数量变多**，但由于之前已经进行过子表的排序，当前的子表**已经接近于正序**，**故此时的排序过程也较快**。因此，希尔排序在整体上要优于直接插入排序。

* 同时，**希尔排序是不稳定的**，并且只能基于**顺序表**实现，因为过程中会通过增量d来快速找到同属于一个子表的元素，表的存储结构必须支持**随机存储**。

* 一般情况下，如果一个排序算法在排序过程中需要**以较大的间隔交换元素**或者**把元素移动一个较大的距离**，那么该算法是**不稳定**的（虽然直接插入排序也有可能移动较大间隔，但是**相同关键字元素的相对顺序和它们被插入到有序区中的顺序是相同的**，而希尔排序是**子表内远距离交换，跳过了中间其他子表元素**）。

# 交换排序

* 所谓交换排序，是指的是**根据序列中两个关键字的比较结果来对换两个记录在序列中的位置，直到没有反序的元素为止**。

### 冒泡排序

* 每一趟冒泡中，**从后往前两两比较相邻元素的值，若为逆序，则交换它们**，这样一趟的结果就是**将无序区中最小的那个元素排到了有序区的末尾**（如果是从前往后冒泡的话，就是将无序区中最大的那个元素排到了有序区的开头）。即一趟排序会**确定一个元素的最终位置**，每趟排序产生的有序区**一定是全局有序区**。
* 在一趟冒泡中，若**没有发生任何的元素交换，就说明此时整个序列就是有序的**，那么可以直接退出循环。
* 冒泡排序同样适用于链表，只是在单链表的情况下只能从头指针开始从左往右冒泡罢了。

### 快速排序

* 快排的基本思想是基于**”分治算法“**的。每一轮从待排序列中选出一个**”基准元素“**，然后通过**双指针交替搜索、元素交换**将序列中**所有大于基准的元素放到它的右边**，**所有小于基准的元素放到它的左边**。这样每趟会将一个元素放到最终位置。然后分别对左右两个区间做相同的操作，直至结束。
* 空间复杂度：由于快排是递归执行的，**空间消耗与递归的最大深度一致。**
  * 最好情况下：递归深度为$log_2{n}$，所以消耗为$O(log_2{n})$。
  * 最坏情况下：此时初始序列为正序或者反序，递归深度为$n-1$，所以消耗为$O(n)$。
  * 平均情况下：消耗为$O(log_2{n})$。
* 时间复杂度：由于快排是递归的，**总的时间消耗为每一层时间消耗相加。**
  * 最好情况下：递归深度为$log_2{n}$，而每一层处理的元素个数都不会超过$n$，因此时间复杂度为$O(nlog_2{n})$。
  * 最坏情况下：此时初始序列为正序或者反序，递归深度为$n-1$，因此时间复杂度为$O(n^2)$。
  * 平均情况下：此时的运行时间与最好情况很接近而不是最坏情况，所以时间复杂度为$O(nlog_2{n})$。
* **快速排序是不稳定的！！！**
* 可以看到，为了提升算法效率，我们可以：
  * **尽量选取一个可以将数据中分的基准元素**，例如从头、中、尾部取三个元素，再取三者中的最大值。
  * **尽量随机选取基准元素（而不是某个下标特定的元素），这样可以使得最坏情况在实际排序中几乎不可能发生。**

```java
public void quickSort(int left, int right, int[] a) {
        if(left >= right)
            return;
        int i = left;
        int j = right;
        int tmp = a[i];
        while(i < j)
        {
            while(i < j && a[j] >= tmp)
                j--;
            a[i] = a[j];
            while(i < j && a[i] <= tmp)
                i++;
            a[j] = a[i];
        }
        a[i] = tmp;

        quickSort(left, i-1, a);
        quickSort(i+1, right, a);
    }
```

# 选择排序

* 每一趟从**无序区中选择一个最小的（或最大的）元素，放在有序区的末尾（或开头）**。这与插入排序不同，插入排序是**每一趟将无序区开头的元素插入到有序区的适当位置**。

### 堆排序

* 堆是一个**数组**，逻辑上可以被看成一棵**完全二叉树的顺序存储结构**，按层次遍历依次存放各个元素。而堆排序就是利用了完全二叉树中，父亲结点和孩子结点之间的位置关系（层次遍历中，**编号为`i`的结点，它的左孩子结点编号为`2i`，它的右孩子结点编号为`2i+1`，而它的父亲结点编号为`⌊i/2⌋`**），从而在无序区中选择关键字最大（或最小）的元素，放进有序区中。

* 在计算机中求编号为`i`的结点的左孩子，可以将`i`**左移一位**快速计算出；求它的右孩子，可以将`i`**左移一位并在低位加一**快速计算出；求它的父亲结点，可以将`i`**右移一位**快速计算出。所以根据这种高效率算法和数组的**随机存取**特性，堆排序的时间消耗非常可观。

* 堆分为大根堆和小根堆：

  * 大根堆：**每个父亲结点都大于等于所有孩子结点的堆**，$A[i] \geq A[2i],A[i] \geq A[2i+1]$，大根堆多用于**堆排序**。

  * 小根堆：**每个父亲结点都小于等于所有孩子结点的堆**，$A[i] \leq A[2i],A[i] \leq A[2i+1]$，小根堆多用于**构造优先队列**。
  * 可以看见，在大根堆中选出最大元素（数组第一个元素或者说二叉树的根节点）并放入有序区是非常容易的，同时在小根堆中选出最小元素也是非常容易的。

* **维护某个结点大根堆的性质（maxHeapify）**：

  * 假设根结点为A[i]的完全二叉树，其**左右孩子结点满足大根堆的特性**，但是A[i]可能小于其孩子结点，这样该结点就违背了大根堆的性质。此时我们就**需要将A[i]、A[2i]、A[2i+1]三个节点中最大的那个上浮到A[i]处**。如果A[i]已经最大，那么以`i`为根节点的树已经是大根堆，否则，**需要将A[i]与其某个孩子结点的值进行交换**。不过在交换后，**A[i]下沉，可能又会破坏下一级子树的大根堆性质，那么就需要对下一级子树递归地调用maxHeapify()来维护该子树的大根堆性质**。
  * 维护大根堆的算法时间消耗主要是调整元素位置以及递归调用的消耗，可以证明，对于一个树高为`h`的结点来说，最多使其下沉**h-1**层，每下降一层，最多进行2次元素比较，所以总共不会超过**2(h-1)**次元素比较，**故而在该结点上调用maxheapify()时间复杂度为$O(h)$，也就是$O(log_2{n})$**。

* **创建一个大根堆（buildMaxHeap）**：

  * 由于在完全二叉树中，**编号1~⌊n/2⌋的结点是分支结点**，之后的全为叶子结点，并且显而易见的是，叶子结点没有孩子结点，故而**自己本身就是一个大根堆**；但分支结点有孩子结点，初始状态下可能不是一个大根堆。所以，建立大根堆，通常需要从下标为⌊n/2⌋的结点开始，直到下标为1的结点，依次对每个结点调用**maxHeapify()**维护其大根堆性质，使整个堆中的**”大者“上浮、“小者”下沉**，最后，根节点就是最大的元素。
  * 我们可以简单估算一下buildMaxHeap()的时间消耗上界：每次调用maxHeapify()的时间复杂度为$O(log_2{n})$，而buildMaxHeap()需要调用⌊n/2⌋次maxHeapify()，消耗时间$O(n)$，所以总的时间复杂度为$O(nlog_2{n})$。但这只是个**简单的估算**并**不是渐进紧确的**。
  * 其实，**不同结点运行maxHeapify()维护大根堆性质的时间与该结点的树高有关，而且大部分的结点高度都很小**，故而可以证明（此处不给），运行buildMaxHeap()的总时间代价为$O(n)$。也就是说，**我们可以在线性时间复杂度内，把一个无序数组构造成一个最大堆**。

* 大根堆排序算法：

  * 在构造出一个大根堆之后，整个数组中最大的元素被放置到了**数组头部**，即**大根堆的根节点**。此时需要**将整个堆的最后一个元素和根节点元素进行互换，那么堆中最大元素就被放到了正确的位置**，然后待排序元素个数减一（其实，**大根堆整体是一个待排序的无序区**，而在数组末尾存在着一个有序区，大根堆排序算法就是不停地、**将有序的大根堆中那个最大元素选择出来，放入有序区**，然后无序区也就是大根堆长度**减一**）。
  * 由于根节点和大根堆末尾元素互换位置，**根节点的大根堆性质被破坏，需要重新维护其大根堆性质**。但需要注意的是，此时**只有根节点一个结点不满足大根堆性质**，而**它的两个子树一定是大根堆**，所以此时**只需要对根节点调用一次maxHeapify()即可，不必对整个堆调用buildMaxHeap()**。
  * 算法总共会遍历n-1次，**每一次都会调用maxHeapify()**，而调用maxHeapify()一次的时间复杂度为$O(log_2{n})$，所以**总的时间复杂度为$O(nlog_2{n})$**。
  * 进行筛选时，可能把后面相同关键字的元素调整到前面，所以堆排序是**不稳定**的。

### 堆排序的其他应用

* 在一个数据不断增多的数据流中，维护数据流中**最小的K个数（或者第K小的数）**：
  * 最简单易懂的方法就是**维护一个数组，当添加新元素时扩容，然后对其排序，求得结果**。但是每次对数组进行排序时，**最快也要消耗$O(Klog_2{K})$的时间复杂度，而且当添加操作次数很多时，消耗巨大**。
  * 进而考虑使用**堆**这种数据结构，因为堆有一个很好的特性，那就是**自身自带排序功能，可以自己维护元素的相关顺序**。
  * 使用**一个长度为K的大根堆**，每次添加新元素时，将新元素和大根堆的堆顶元素进行比较：
    * 若新元素大于堆顶元素，说明**最小的K个数中、那个最大的数，都没有新元素大**，进而说明新元素不可能被包含于最小的K个数中。
    * 若新元素小于堆顶元素，说明**最小的K个数中、那个最大的数，大于新元素**，进而说明新元素应该将堆顶元素挤出堆外，**新元素应成为最小的K个数之一**。此时只需要将堆顶元素换为新元素，然后**维护堆顶的大根堆性质**即可。
  * 为什么使用大根堆而不使用小根堆？
    * 使用大根堆时，**堆顶元素就是第K小的数**，而每次替换元素时，**只需要替换掉堆顶元素**，然后也**只需要维护堆顶（也就是堆的根节点）的大根堆性质**。
    * 而使用小根堆时，**堆顶元素是整个数据集中最小的数，至于第K小的数却在堆的下方某处**。也就是说，在替换元素时，需要**先找到第K小的数的下标**，然后替换掉它，这同时也会导致**需要对K/2个结点进行小根堆性质维护**，消耗巨大。
* 在一个数据不断增多的数据流中，维护数据流中**最大的K个数（或者第K大的数）**：
  * 和上文一样，使用堆要比单纯的数组排序快捷，不过换成了**小根堆**而已，其余原理完全一样。

# 归并排序

* **归并排序的主要思想就是将两个或两个以上的有序表组合成一个新的有序表，有几个有序表就是几路归并。**
* **合并两个前后相邻的有序表merge()**：
  * 每次从两个有序区间中取最小的元素，放进新数组的有序区中，一直下去，**直至某个区间为空**，此时就可以直接将另外一个区间的剩余元素复制到新数组即可。
  * 当两个子区间都扫描到**相同元素**时，应**优先将左边区间的元素放入到新数组有序区**中，这样可以保证算法的**稳定性**。
* 每趟归并时间复杂度为$O(n)$，总共需要进行$log_2{n}$趟归并，所以**算法总时间复杂度为$O(nlog_2{n})$**。

# 两个集合的不同元素个数（两个集合的差集）

* 计算集合$A$与集合$B$不同元素的个数，可以使用Map字典，存放**$A$集合中每个元素的个数**，然后对于$B$中每个元素，使其在字典中的数量减一，这样最后，字典中**数量大于0的元素数量，就是$A-B$的大小，而小于0的的元素数量，就是$B-A$的大小**。
