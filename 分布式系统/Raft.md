# Raft基本概念

### 节点角色

* Raft集群中的节点分为三个类型：

  * **Leader**
  * **Candidate**
  * **follower**

  ![raft-server-states](https://img.draveness.me/2018-10-22-raft-server-states.png)

* 所有的 Follower 节点都是**被动**的，它们**不会主动发出任何的请求，只会响应 Leader 和 Candidate 发出的请求**。

* 对于每一个用户的可变操作，都会**被路由给 Leader 节点**进行处理，除了 Leader 和 Follower 节点之外，Candidate 节点其实只是集群运行过程中的一个**临时状态**（是在**领导人选举**过程中的一个临时状态）。

### 任期（term）

* Raft算法将时间划分为**任意个不同长度的任期**，任期是单调递增的，用连续的数字（1，2，3······）表示。
* 每一个任期的**开始**都是**一次领导人的选举**（注意，不是选出了leader这时才会导致一个任期的开始，而是当一个选举过程开始时，一个任期就已经开始了），同样一次领导人的选举也是**上一个任期的结束**。
* 若一个任期内，各个candidate的选票被瓜分，**没有选出最终的leader**，那么本次任期**将以没有选出leader而结束**，系统将自动进入下一个任期，开始一次新的领导人选举。
* 在分布式系统中，**“时间同步”**是一个大问题，而某些时候为了识别**“过期信息”**，时间信息有是必不可少的要素。而在Raft协议中，任期便充当着**“逻辑时钟”**的角色，也被用于在Raft中**检测过期信息——如过期的leader。**
* 每个Raft节点各自都在本地维护一个当前任期值，触发这个数字变化（增加）主要有两个场景：
  * 开始选举：若该节点**在超时限制内都没有收到来自leader的心跳信息**，那么该节点将从follower变为candidate，**任期自动加一。**
  * 与其他节点交换信息：当节点之间进行通信时，会相互交换当前的任期号。如果**任意一个节点（包括领导人）**的当前任期号比其他节点的任期号小，则**将自己本地的任期号自觉地更新为较大的任期号**；如果**一个候选人或者领导人**意识到它的任期号过时了（比别人的小），那么它会**立刻切换回群众状态**；如果一个节点收到的**请求**所携带的任期号是过时的，那么该节点就会拒绝响应本次请求。

# 领导人选举

### 至高无上的领导人

* Raft采用的是**强领导制**，每一个通过选举得出的领导人都拥有**至高无上的权力**。并且系统赋予他管理复制日志的重任，来维护节点间复制日志的一致性：领导人**从客户端接收日志条目**，再**把日志条目复制到其他服务器**上，并且在保证安全性的前提下，告诉其他服务器将日志条目应用到它们的状态机中。
* 强领导人的存在大大简化了复制日志的管理：
  * 领导人可以**决定新的日志条目需要放在日志文件的什么位置**，而**不需要和其他服务器商议**。
  * 数据都是**单向地从领导人流向其他服务器**。

### 心跳和选举计时器

* 每个Raft节点都有一个**选举定时器**，所有的Raft节点最开始以Follower角色运行时，都会启动这个选举定时器。不过，**每个节点的选举定时器时长均不相等。**
* Leader在任期内必须**定期**向集群内的其他节点**广播心跳包（一个空的AppendEntries RPC）**，**向天下昭告自己至高无上的存在**：
  * Follower每次收到心跳包后就会主动**将自己的选举定时器清零重置（reset）**
  * 如果Follower选举定时器**超时**，则意味着在Raft规定的一个选举超时时间周期内，**Leader的心跳包并没有及时到达Follower**（有可能是已经发送了但在**网络传输过程中发生了延迟或被丢弃了**，也有可能是Leader**直接宕机或发生了故障**），于是Follower就**假定Leader已经不存在或者发生了故障**，接着便以Candidate的身份**发起一次新的选举**。
* 所以，为了不让follower频繁的发起选举，**Leader的心跳广播周期**必须**小于节点选举计时器的超时时间**，好让节点有足够的时间接受来自Leader的心跳信息。

### 发起选举

* 如果一个Follower决定开始选举，它会执行以下步骤：
  * **将自己任期号（current_term_id）加1**
  * 将**状态切换到候选人（Candidate）**，并**为自己投票**（也就是说每个候选人的**第一张选票来自于他自己**，这同时也说明了，存在着多个候选人**同时发起选举导致选票被平分**，从而选举失败的情况）
  * 向集群中的其他节点发送**RequestVote RPC**（RPC消息会携带“current_term_id”值），进行拉票。
* 一个Follower会按照以下规则进行**投票**：
  * 在任一任期内，单个节点最多只能投一票
  * 候选人知道的信息不能比投票者少
  * 先来先得
* 一个Candidate在发起选举后，有三种状态迁移的可能性：
  * 得到**大多数节点**的选票（包括自己）：
    * Candidate如果在一个任期内收到了集群中大多数Follower的投票，就算赢得了选举。赢得了选举之后，它就会作为新的Leader向其他节点发送心跳信息来建立自己的领导地位，广而告之，避免其余节点触发新的选举。
  * 发现**其他节点赢得了选举**：
    * Candidate在等待其他人的选票时，它有可能会收到来自**其他节点的、声称自己是Leader的AppendEntries RPC**
    * 此时，这个Candidate会**将信将疑**地检查包含在这位Leader的RPC中的任期号：
      * 如果比自己本地维护的当前任期要大，则承认该领导人合法，并且主动将自己的状态切换回Follower；
      * 反之，候选人则认为该“领导人”不合法，拒绝此次RPC，并且返回当前较新的那个任期号，以便让“领导人”意识到自己的任期号已经过时了，该节点将继续保持候选人状态不变。
  * 过了一段时间后，发现没有其余节点赢得选举，自己也未获得多数选票：
    * 可能某个任期内有**多个Candidate**，它们**瓜分了选票**，并且**没有任何节点获得多数节点的支持**进而成为Leader
    * 针对这种情形，每一个Candidate内都设置了一个**超时时钟**（类似于Follower的选举超时时钟），当发生超时后，Candidate将默认该任期内没有Leader产生，将**自身任期加一**，**开启下一个任期的选举**
    * 很显然，下一个任期的选举也有可能出现相同的情况（其实理论上将，**选票均分的情况可能会无限次发生**，这也和FLP不可能定理相对应）。
    * 所以Raft采用了**随机重试**的办法：设置一个**区间（150~300ms）**，**各个Candidate的超时时间将在该区间内随机选择**。这样，各个Candidat**e重新发起竞选的时间窗口将会被错开**，使得大多数情况下，**只有一个节点会率先超时**，然后该节点**在其余节点的、上一任期超时前，赢得下一任期的选举。**

# 日志复制

### 日志条目的相关属性

* **整数索引**：即该条目在日志文件中的槽位
* **任期号**：即该条目被Leader创建时所处的任期号，**用来检测在不同服务器上日志的不一致性**
* **指令**：即用于被状态机执行的指令

![img](https://knowledge-sharing.gitbooks.io/raft/content/assets/Figure-6-Logs.png)

​	途中**条目7**被集群多数节点所复制，所以可以被提交；而**条目8**只被两个节点所复制，所以不可提交。

### 日志的提交

* 由**Leader**决定什么时候将日志条目应用到状态机，即**提交日志**。
* 一旦Leader创建的日志条目已经被复制到**半数以上**的节点上了，那么这个日志条目就称为**可提交的（commited）**。
* 一旦Leader中提交了某个日志，那么也会提交**该日志之前的所有日志**，包括之前的领导人创建的日志条目。
* Leader会追踪记录它所知道的**被提交日志条目**的**最大索引值（commitIndex）**，并且将该索引值包含在向其他节点发送的**AppendEntries RPC**中，以告知其余节点**该索引值对应的日志条目已经被提交**。
* 若一个Follower发现某个索引的日志**已经被Leader提交了**，那么自己也会**将该日志应用到本地状态机中。**

### 日志匹配原则

* Raft保证以下特性，用于构成**日志匹配原则**（`Log Matching Property`）：

  * 如果在不同日志中的两个条目有着相同的索引和任期号，则它们**所存储的命令是相同的。**
  * 如果在不同日志中的两个条目有着相同的索引和任期号，则它们**之前的所有条目都是完全一样的。**

  第一条特性是因为一个Leader**在一个任期内的给定的一个索引位置**，只能创建**一个日志条目**，并且该日志条目**在日志文件中的位置永远不会改变。**

  第二条特性则是因为使用**AppendEntries RPC**进行**简单一致性检查**。

### 产生新Leader后的日志一致性冲突

* 当一个新的Leader被选举出来时，**它的日志与其他的Follower的日志可能是不一样的**：

  ![img](https://pdf.cdn.readpaper.com/parsed/fetch_target/b66ba96f06a2f9ebd01f84632a658a61_7_Figure_7.png)

  **In search of an understandable consensus algorithm，Figure-7**

  上图中列出了几种情况：

  * Follower可能**缺少日志条目**（a-b）
  * Follower可能**有额外的未提交日志条目**（c-d）
  * 或者**两者都有**（e-f）

### 通过AppendEntries RPC进行递归一致性检查

* 为了进行一致性检查，需要涉及到几个数值：
  * 为了使Follower的日志和Leader的日志保持一致，**Leader需要清楚每一个Follower的当前最新日志在何处**，故而Leader在本地为每一个其余Follower维护了一个**`nextIndex`**值，其表示Leader将要发送给该Follower的**下一条新日志条目的索引**。
  * Leader在通过AppendEntries RPC向其余节点追加新日志条目时，会将**新日志条目之前一个槽位的日志条目**的**任期号和索引位置**：二元组$(prevTerm,prevIndex)$包含在消息体中
* 一致性检查的大致过程：
  * 当一个Leader赢得选举时，它会假设每个Follower上的日志都与自己的保持一致，于是先将**每一个nextIndex**初始化为它**最新的日志条目索引数+1**
  * 当Leader向Follower发送AppendEntries RPC包时，数据包中：
    * `prevIndex`是**该Follower的`nextIndex-1`** 
    * `prevTerm`是**`prevIndex`所处日志的任期号**
  * Follower收到数据包后，**判断本地日志中是否存在着和包中的二元组$(prevTerm,prevIndex)$一致的日志条目**：
    * 若Follower在本地没有找到这样的日志，那么它会拒绝这次RPC，返回失败信息
    * 若Follower在本地找得到这样的日志，那么就说明在`prevIndex`这个索引位置上的日志条目，Follower和Leader保持一致（**进而该索引之前的所有日志条目都和Leader保持一致**）。Follower连续删除**该日志索引之后所有的日志条目**，为后续添加Leader新日志做准备（这就是**强领导人**的体现之处，当Leader和Follower之间有日志冲突时，Follower必须无条件遵循Leader的安排）
  * Leader在收到来自Follower的返回信息后：
    * 若成功，说明**Leader和Follower的日志已经达成一致**，并且在该任期的剩余时间内，会保持这种一致性。随后Leader将会从该Follower的$nextIndex$索引处开始，**将之后的所有日志一次性地发送给Follower**
    * 若失败，就意味着Follower发现自己的日志与领导人的不一致。在失败之后，领导人会**将$nextIndex$递减**$(nextIndex--)$，然后**重试AppendEntries RPC，直到返回成功为止**

### 日志复制的大致流程

* 一旦某个Leader赢得了选举，那么它就会开始接收客户端的请求：

  * 客户端向Leader发送**写请求**。
  * 每一个**客户端写请求**都将被解析成一条**可被复制状态机执行的指令**
  * Leader将指令**作为日志条目加入它的日志文件中**，然后向其他Raft节点调用**AppendEntries RPC**，要求其他节点**复制**日志条目。
  * Follower接收到**AppendEntries RPC**后**进行一致性检查**，选择同Leader一致的索引位，从该位置开始追加Leader的日志条目。
  * 当日志条目被**“安全地”**复制（即集群中**大多数节点**都接收到了Leader的复制命令，并且都向Leader进行了回复）之后，Leader会将日志条目**提交**到它的状态机中，并且**向客户端返回执行结果**。
  * 一旦日志提交成功，Leader就将该日志条目对应的指令**应用（apply）**到本地状态机，并向客户端返回操作结果。
  * 当后续Leader向集群发送AppendEntries RPC时，它会把自己**已应用该日志条目**的情况附加进去，**通知Follower应用该日志条目**。
  * Follower收到提交的日志项之后，将其应用至本地状态机。

  ![raft](https://hjlarry.github.io/docs/other/raft/images/raft_summary.png)

* 如果Follower发生错误，运行缓慢**没有及时响应AppendEntries RPC**，或者发生了网络丢包的问题，那么Leader会**无限地重试AppendEntries RPC（甚至在它响应了客户端之后）**，直到所有的Follower最终存储了和Leader一样的日志条目。

* Leader虽然会对Follower上的日志进行各种操作，但**Leader从不会删除或修改自己的日志，只会进行追加操作**。

* 可以看出，针对Raft日志条目有两个操作，**提交（commit）**和**应用（apply）**，**应用必须发生在提交之后**，即某个日志条目只有被提交之后才能被应用到本地状态机上。

# 安全性

### 当前选举规则的缺陷

* 到目前为止Raft的选举机制还**不能保证**每一个状态机都能够**按照相同的顺序执行同样的指令**。例如：
  * 当Leader正在复制日志条目时一个Follower发生了故障，且故障发生之前没有复制领导人的日志，之后该Follower重启并且当选为领导人，那么它在产生了一些新的日志条目后，会用自己的日志覆盖掉其他节点的日志。这就会导致不同的状态机可能执行不同的指令序列。
* 这其实是**对Leader选举提出了资格要求**：选出的Leader必须拥有**全部已被提交的日志**

### Leader选举和日志复制中的数学结论

* 结论一：
  * 一个Candidate必须获得集群中大多数节点支持（包括自己），才能成为Leader。记对一个Candidate投票的大多数节点组成**集合$A$**
  * Leader在进行日志提交前，都必须与集群中大多数节点进行通信，命令这些节点复制日志，所以对于**每一条已经提交的日志条目**，该日志肯定会出现在集群的大多数节点上。记对于日志**$Entry_i$**而言，其对应的大多数节点为**集合$B_i$**
  * 很显然，**$A$与每一个$B_i$之间必然有交集**，即：在进行选举时，**每一条已经提交的日志$Entry_i$，都会出现在$A$中至少一个节点上。**
* 结论二：
  * 由结论一可得：对于**最新被提交的日志$Entry_{max}$**而言，肯定**会出现在A中至少一个节点上**，记该节点为$Node_{latest}$，其**拥有所有已经提交的日志条目**
  * 进而，若**一个Candidate的日志比$A$中所有节点都更新（至少一样新）**，那么该Candidate就是$Node_{latest}$，就**可以被选举为Leader**
* 结论二其实是**对Leader选举提供了可行性证明**：在进行选举时，在为Candidate投票的那些大多数节点中，**至少一个节点拥有成为Leader的资格**

### 选举限制规则

* 在投票时，**若Candidate的日志没有投票者的新，那么投票者将拒绝为该Candidate投票**，这样就**限制了那些没有包括所有已提交日志条目的节点赢得选举**
* 根据**最后一个日志条目**的**索引**和**任期号**判断两份日志的新旧程度：
  * 如果两者任期号不同，则**任期号大的日志较新**
  * 如果两者任期号相同，则**索引值更大（即日志文件条目更多）的日志较新**
* Raft使用该方法保证**每一个新选出的Leader都拥有之前任期的所有日志**，而**不需额外的操作**（例如额外的数据传输）

### 判断日志是否被提交

* 正如上文所述，对于某一条**当前任期的日志条目**，当它被存储到了集群的大多数节点上，那么就**可认为该日志条目被提交**；但是对于某一条**之前任期的日志条目**，当前任期Leader**不能因为其被存储在大多数节点上了，就笃定该日志条目已经被提交了**

  ![img](https://pdf.cdn.readpaper.com/parsed/fetch_target/b66ba96f06a2f9ebd01f84632a658a61_8_Figure_8.png)

  如上图：

  * (a)时刻，S1是第2任期中的Leader，它**将位于2号索引位的日志复制到了S2上**（注意没有复制到大多数节点上）
  * (b)时刻，S1宕机，S5获得S3、S4的选票当选为第3任期的Leader，然后**将另外一个日志条目也追加到了2号索引位**
  * (c)时刻，S5宕机，S1拉起，重新当选为第4任期的Leader，**将第2任期中的2号索引位日志复制到大多数节点（S1，S2，S3）**。此时：
    * (d)时刻，若S1宕机，S5获得S2、S3、S4的选票当选为Leader，然后**重写覆盖集群中的第2任期2号索引位的日志**
    * (e)时刻，若S1在宕机之前，**将第4任期的3号索引位日志复制到大多数节点**，那么S1宕机后，**S5也无法当选为Leader，从而2号索引位的日志也无法被重写覆盖**

* Raft不会通过判断是否复制到大多数节点来提交之前任期的日志，**只能提交当前任期的日志**。而**根据日志匹配原则**，只要当前任期的日志被提交，那么**之前所有任期的日志都自动地被提交了**

# 时序与可用性

* 设计Raft的要求之一就是**系统的正常运行不能依赖于时序**：不能因为某些事件的发生快慢不同就产生不同的结果甚至是错误
* 但是系统的可用性又不可避免地依赖于时序

